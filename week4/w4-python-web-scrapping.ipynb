{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Part: Notes on genism, bs, and word2vec\n",
    "link: [github session notes link with core libraries](https://github.com/ual-cci/MSc-Coding-2/blob/master/Week-4.md)\n",
    "\n",
    "- tip: use help() and dir()\n",
    "- urllib: https://pythonspot.com/urllib-tutorial-python-3/\n",
    "- bs4: parsing html; use it when grabbing webpage data\n",
    "- matplotlib\n",
    "- numpy\n",
    "- genism\n",
    "\n",
    "### Genism\n",
    "- a general purpose Topic modelling and natural language processing library(machine learning)\n",
    "- auto-summarisation, sentiment analysis, word-vectors, and so on\n",
    "- https://radimrehurek.com/gensim/\n",
    "\n",
    "#### Word2Vec\n",
    "- Word2Vec approach uses deep learning and neural networks-based techniques to convert words into corresponding vectors\n",
    "- in such a way that the semantically similar vectors are close to each other in N-dimensional space, \n",
    "- where N refers to the dimensions of the vector.\n",
    "- Word2Vec returns maintain semantic relation. Ex:King - Man + Women = Queen\n",
    "- More info in [this tutorial](https://stackabuse.com/implementing-word2vec-with-gensim-library-in-python/)\n",
    "- Pro:\n",
    "    - retains the semantic meaning of different words in a document.\n",
    "    - the size of the embedding vector is very small\n",
    "    \n",
    "#### NLTK Tokenize\n",
    "- NLTK Tokenize: Words and Sentences Tokenizer\n",
    "- Tokenization is the process by which a large quantity of text is divided into smaller parts called tokens. These tokens are very useful for finding patterns and are considered as a base step for stemming and lemmatization.\n",
    "- Natural language processing is used for building applications such as Text classification, intelligent chatbot, sentimental analysis, language translation, etc.\n",
    "- related example and tutorial: [link here](https://www.guru99.com/tokenize-words-sentences-nltk.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second part - The Exercise\n",
    "- Build a simple webscraper that scrapes a set of documents from the internet and summarises them using gensim.\n",
    "- If you manage to achieve this, extract keywords from all the different documents and see if any are more popular than others.\n",
    "- Search for documents that contain those keywords using Python and then summarise those documents too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary\n",
      "Machine learning algorithms build a model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so.[2] Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.\n",
      "Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[19]:488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[20] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[19]:708â€“710; 755 Neural networks research had been abandoned by AI and computer science around the same time.\n"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "#use urllib's urlopen to get the webpage and read the article.\n",
    "scrapped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Machine_learning')\n",
    "article = scrapped_data .read()\n",
    "\n",
    "#create a bs object to parse the data.\n",
    "#remember to install lxml beforehand. It's used for parsing XML and HTML\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "#because text content normally is wrapped in <p> tag in html. so use bs's findall() to get the content inside p tag.\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "article_text = \"\"\n",
    "for p in paragraphs:\n",
    "    article_text += p.text\n",
    "#print(article_text) #we get an article now.\n",
    "\n",
    "# we summarize the text\n",
    "from gensim.summarization import summarize\n",
    "mySummary = summarize(article_text,word_count=150)\n",
    "print (\"Summary\")\n",
    "print (mySummary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find Similar Word of 'artificial'\n",
      "[('training', 0.4113386869430542), ('rules', 0.39969879388809204), ('methods', 0.3768565058708191), ('program', 0.37293824553489685), ('intelligence', 0.3681725859642029), ('trained', 0.36725664138793945), ('hiring', 0.36059659719467163), ('difficult', 0.3424645960330963), ('inference', 0.3359124958515167), ('field', 0.3338709771633148)]\n"
     ]
    }
   ],
   "source": [
    "#Find similar word using nltk tokenization and Word2Vec model\n",
    "\n",
    "\n",
    "# clean the text, remove any extra space or special charaters.\n",
    "# also make all the text lower case\n",
    "# to make it only containing words, convenient for word2vec use.\n",
    "# use RegEx's sub() to replace the mateches with space\n",
    "processed = article_text.lower()\n",
    "processed = re.sub('[^a-z]', ' ', processed )#[^a-zA-Z]means any characters EXCEPT lowercase\n",
    "processed = re.sub('\\s+', ' ', processed)#\\s means any string DOES NOT contain a white space character. This step is to remove extra space.\n",
    "\n",
    "#use nltk.sent_tokenize to convert our article into sentences.\n",
    "all_sentences = nltk.sent_tokenize(processed)\n",
    "# print(all_sentences)\n",
    "\n",
    "#To convert sentences into words,  use nltk.word_tokenize.\n",
    "all_words = []\n",
    "for sent in all_sentences:\n",
    "    all_words.append(nltk.word_tokenize(sent))\n",
    "# one line code: all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "#now we have a list with each word as string.\n",
    "\n",
    "#but there are many unnecessary words. such as \"is\" \"the\" \"by\"\n",
    "# I need to remove these words.\n",
    "# there words are named \"stop words\" \n",
    "# more info here:https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]\n",
    "    \n",
    "\n",
    "#Creating Word2Vec model\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "#create the model with minCount as 2, it specifies to include only those words in the Word2Vec model that appear at least twice in the corpus.\n",
    "word2vec = Word2Vec(all_words, min_count=2)\n",
    "\n",
    "# the Word2Vec model converts words to their corresponding vectors. \n",
    "# v1 is the vector of the word artificial\n",
    "v1 = word2vec.wv['artificial']\n",
    "# find all the words similar to the word \"intelligence\"\n",
    "sim_words = word2vec.wv.most_similar('artificial')\n",
    "print(\"Find Similar Word of 'artificial'\")\n",
    "print(sim_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'find'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-fb1e6b0ae9f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'li'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlists_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# for link in soup.find_all('a'):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/bs4/element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2172\u001b[0m         \u001b[0;34m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2173\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   2174\u001b[0m             \u001b[0;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2175\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'find'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "\n",
    "# just get all the links. Links are 'a' (as in <a href = \"\">)\n",
    "page = urllib.request.urlopen('https://en.wikipedia.org/wiki/Category:Machine_learning').read()\n",
    "soup = bs.BeautifulSoup(page)\n",
    "# soup = BeautifulSoup(page.content, 'html.parser')\n",
    "# just get all the links. Links are 'a' (as in <a href = \"\">)\n",
    "\n",
    "lists = soup.find_all('li')\n",
    "lists_link = lists.find('a')\n",
    "print(lists)\n",
    "# for link in soup.find_all('a'):\n",
    "#     print(link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
